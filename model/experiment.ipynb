{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxliu2/anaconda3/envs/sp23/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "import transformers\n",
    "import lightning.pytorch as pl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "target_json_path = \"../data/json/data_coarse.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buf_str    Jos√© Ignacio Hualde\n",
      "lbuf                 42.050588\n",
      "rbuf                 58.439608\n",
      "stk_str                  $ROOT\n",
      "lstk                       0.0\n",
      "rstk                     100.0\n",
      "type               subordinate\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(target_json_path)\n",
    "print(df.iloc[1])\n",
    "\n",
    "all_labels = sorted(list(set(df['type'].tolist())))\n",
    "n_classes = len(all_labels)\n",
    "label_idx = {lab: int(i) for i, lab in enumerate(all_labels)}\n",
    "idx_label = {int(i): lab for i, lab in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer_args = {\n",
    "    'padding': 'max_length',\n",
    "    'return_tensors': 'pt',\n",
    "}\n",
    "\n",
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.df = pd.read_json(data_dir)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        element = self.df.iloc[idx]\n",
    "        pos = torch.floor(torch.Tensor([element.lbuf, element.rbuf, element.lstk, element.rstk])).long()\n",
    "        return element.buf_str, element.stk_str, pos, label_idx[element.type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dataset = ResumeDataset(target_json_path)\n",
    "total_count = len(resume_dataset)\n",
    "train_count = int(0.85 * total_count)\n",
    "valid_count = int(0.1 * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    resume_dataset, (train_count, valid_count, test_count)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_of_tensors_to_tensor(tuple_of_tensors):\n",
    "    # https://discuss.pytorch.org/t/convert-a-tuple-into-tensor/82964\n",
    "    return  torch.stack(list(tuple_of_tensors), dim=0)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    buf_str, stk_str, pos, label_idx = zip(*batch)\n",
    "    # print(\"pos type: \", type(pos), \"contents: \", pos)\n",
    "    # print(\"label_idx type: \", type(label_idx), \"contents: \", label_idx)\n",
    "    buf_str = list(buf_str)\n",
    "    stk_str = list(stk_str)\n",
    "    buf_emb = tokenizer(buf_str, **tokenizer_args) \n",
    "    stk_emb = tokenizer(stk_str, **tokenizer_args) \n",
    "    return buf_emb, stk_emb, tuple_of_tensors_to_tensor(pos), torch.tensor(list(label_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn = collate_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'positional_dim': 32,\n",
    "    'hidden_dim': 256,\n",
    "    'classifier_dropout': 0.3,\n",
    "    'num_classes': n_classes,\n",
    "    'n_hidden': 1, # total layers: n_hidden + 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/wzlxjtu/PositionalEncoding2D/blob/master/positionalembedding2d.py\n",
    "import math\n",
    "def positionalencoding1d(d_model, length):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the model\n",
    "    :param length: length of positions\n",
    "    :return: length*d_model position matrix\n",
    "    \"\"\"\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dim (got dim={:d})\".format(d_model))\n",
    "    pe = torch.zeros(length, d_model)\n",
    "    position = torch.arange(0, length).unsqueeze(1)\n",
    "    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                         -(math.log(10000.0) / d_model)))\n",
    "    pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Dropout, ReLU, Embedding, CrossEntropyLoss\n",
    "import math \n",
    "\n",
    "class ResumeParser(pl.LightningModule):\n",
    "    def __init__(self, backend, args):\n",
    "        super().__init__()\n",
    "        self.backend = backend \n",
    "        self.classifier = nn.Sequential(\n",
    "            Linear(in_features = self.backend.config.hidden_size + 4 * args['positional_dim'], out_features = args['hidden_dim']),\n",
    "            Dropout(p = args['classifier_dropout']),\n",
    "            ReLU(),\n",
    "            Linear(in_features = args['hidden_dim'], out_features = args['hidden_dim']), #n_hidden = 1 hardcoded\n",
    "            Dropout(p = args['classifier_dropout']),\n",
    "            ReLU(),\n",
    "            Linear(in_features = args['hidden_dim'], out_features = args['num_classes']),\n",
    "            Dropout(p = args['classifier_dropout']),\n",
    "        )  \n",
    "        # self.pos_embeddings = Embedding(num_embeddings = 100, embedding_dim = args['positional_dim'])\n",
    "        self.pos_embeddings = positionalencoding1d(args['positional_dim'], 101)\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.metric = Accuracy(task = \"multiclass\", num_classes = n_classes)\n",
    "        self.running_loss = None\n",
    "\n",
    "        self.ce_loss = CrossEntropyLoss()\n",
    "        \n",
    "    def get_logits_and_loss(self, batch):\n",
    "        inp_buf, inp_stk, pos, typ = batch \n",
    "        pos_emb = self.pos_embeddings[pos.cpu()].to(self.device) # B x 4 x D_pos\n",
    "        pos_emb = pos_emb.reshape((-1, 4 * args['positional_dim'])) # concatenate all positional embeddings\n",
    "        # print(\"pos_emb before shape: \", pos_emb.shape)\n",
    "        # pos_emb = pos_emb.sum(dim = 1) # sum the positional embeddings (B x D_pos)\n",
    "        # print(\"pos_emb after shape: \", pos_emb.shape)\n",
    "        # print(\"inp ids shape:\", inp_buf['input_ids'].shape, \"inp_buf type: \", type(inp_buf))\n",
    "        emb_buf = self.backend(**inp_buf)['pooler_output'] # B x D_backend\n",
    "        emb_stk = self.backend(**inp_stk)['pooler_output'] # B x D_backend\n",
    "        # print(\"Pos embedding shape: \", pos_emb.shape, \", emb_buf shape: \", emb_buf.shape, \" emb_stk shape: \", emb_stk.shape)\n",
    "        classifier_inp = torch.cat((emb_buf + emb_stk, pos_emb), 1) # B x (D_backend + D_pos)\n",
    "        logits = self.classifier(classifier_inp)\n",
    "        loss = self.ce_loss(logits, typ)\n",
    "        return logits, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, loss = self.get_logits_and_loss(batch)\n",
    "        \n",
    "        if self.running_loss == None:\n",
    "            self.running_loss = loss\n",
    "        self.running_loss = 0.95 * self.running_loss + 0.05 * loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, _, _, typ = batch \n",
    "        logits, loss = self.get_logits_and_loss(batch)\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        # print(\"logits shape:\", logits.shape, \", preds.shape: \", preds.shape)\n",
    "        \n",
    "        self.log(\"Validation Accuracy\", self.metric(preds, typ))\n",
    "        self.log(\"Validation Loss\", loss)\n",
    "        if self.running_loss is not None:\n",
    "            self.log(\"Training Loss\", self.running_loss)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters())\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ResumeParser(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.cuda.device object at 0x7ff36c226e20>, <torch.cuda.device object at 0x7ff36c226a60>, <torch.cuda.device object at 0x7ff36c226a00>]\n"
     ]
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6bc28c91f633551b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6bc28c91f633551b\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=[1], val_check_interval = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxliu2/anaconda3/envs/sp23/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | backend    | BertModel          | 108 M \n",
      "1 | classifier | Sequential         | 296 K \n",
      "2 | metric     | MulticlassAccuracy | 0     \n",
      "3 | ce_loss    | CrossEntropyLoss   | 0     \n",
      "--------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "434.427   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxliu2/anaconda3/envs/sp23/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxliu2/anaconda3/envs/sp23/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 207/311 [03:00<01:30,  1.15it/s, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxliu2/anaconda3/envs/sp23/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(parser, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_workers / it/s\n",
    "# 0: 1.33\n",
    "# 16: 1.25\n",
    "# 40: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff3b2c9d663614f3977b65735ceb4b5f1b8eea6ba2762bc66636352610c8026c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
