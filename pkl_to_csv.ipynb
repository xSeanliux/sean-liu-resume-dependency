{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"data/pkl_fine\"\n",
    "in_file = \"cv.pkl\"\n",
    "out_dir = \"data/json/\"\n",
    "out_file = \"data_fine.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = f'{in_dir}/{in_file}'\n",
    "out_path = f'{out_dir}/{out_file}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_wrapper import PDFWrapper\n",
    "def get_record(in_path):\n",
    "    with open(in_path, \"rb\") as f_in, open(out_path, \"w\") as f_out:\n",
    "        data = pkl.load(f_in)\n",
    "        json_format = data.json_format\n",
    "        pdf_path = in_path.replace(\"pkl_fine\", \"pdf\")\n",
    "        pdf_path = pdf_path.replace(\".pkl\", \".pdf\")\n",
    "        wrapper = PDFWrapper(fname = pdf_path)\n",
    "        lines = wrapper.lines\n",
    "        for record in tqdm(data.record):\n",
    "            # print(record)\n",
    "            buf_idx = record['from']\n",
    "            stk_idx = record['to']\n",
    "            type_str = record['type']\n",
    "            buf_string = None\n",
    "            stk_string = None\n",
    "            lbuf = None\n",
    "            rbuf = None \n",
    "            lstk = None \n",
    "            rstk = None \n",
    "            hstk = None\n",
    "            boldbuf = None\n",
    "            italbuf = None\n",
    "            boldstk = None\n",
    "            italstk = None\n",
    "            hbuf = None\n",
    "            if(buf_idx == -1):\n",
    "                buf_string = \"$ROOT\"\n",
    "                lbuf = 0\n",
    "                rbuf = 100\n",
    "                hbuf = 30\n",
    "                boldbuf = 0\n",
    "                italbuf = 0\n",
    "            else:\n",
    "                buf_string = \"$ROOT\" if buf_idx == -1 else json_format[buf_idx]['text']\n",
    "                lbuf = json_format[buf_idx]['x']\n",
    "                rbuf = json_format[buf_idx]['x'] + json_format[buf_idx]['width']\n",
    "                hbuf = int(json_format[buf_idx]['height'])\n",
    "                try:\n",
    "                    linebuf = lines[json_format[buf_idx]['page']][json_format[buf_idx]['idx_in_page']]\n",
    "                except:\n",
    "                    print(f\"Tried to get line #{json_format[buf_idx]['idx_in_page']} of page {json_format[buf_idx]['page']}; document has {len(wrapper.elements)}/{len(wrapper.lines)} pages, and that page has {len(lines[json_format[buf_idx]['idx_in_page']])} lines\")\n",
    "                    raise KeyError\n",
    "                fontname = linebuf._objs[0].fontname.lower()\n",
    "                boldbuf = 1 if \"bold\" in fontname else 0\n",
    "                italbuf = 1 if \"italic\" in fontname else 0\n",
    "\n",
    "            if(stk_idx == -1):\n",
    "                stk_string = \"$ROOT\"\n",
    "                lstk = 0\n",
    "                rstk = 100\n",
    "                hstk = 30\n",
    "                boldbuf = 0\n",
    "                italbuf = 0\n",
    "            else:\n",
    "                stk_string = \"$ROOT\" if stk_idx == -1 else json_format[stk_idx]['text']\n",
    "                lstk = json_format[stk_idx]['x']\n",
    "                rstk = json_format[stk_idx]['x'] + json_format[stk_idx]['width']\n",
    "                hstk = int(json_format[stk_idx]['height'])\n",
    "                linebuf = lines[json_format[stk_idx]['page']][json_format[stk_idx]['idx_in_page']]\n",
    "                fontname = linebuf._objs[0].fontname.lower()\n",
    "                boldbuf = 1 if \"bold\" in fontname else 0\n",
    "                italbuf = 1 if \"italic\" in fontname else 0\n",
    "            yield {\n",
    "                'buf_str': buf_string,\n",
    "                'lbuf': lbuf,\n",
    "                'rbuf': rbuf,\n",
    "                'hbuf': hbuf,\n",
    "                'boldbuf': boldbuf,\n",
    "                'italbuf': italbuf,\n",
    "                'stk_str': stk_string,\n",
    "                'lstk': lstk,\n",
    "                'rstk': rstk,\n",
    "                'boldstk': boldstk,\n",
    "                'italstk': italstk,\n",
    "                'type': type_str,\n",
    "                'hstk': hstk\n",
    "            }\n",
    "            # print(f'{buf_string},{stk_string},{type_str}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fix a bug where the discard function had the from and to indices both point to themselves \n",
    "\n",
    "# def fix_discard(in_path):\n",
    "    \n",
    "#     data = None\n",
    "#     print(in_path)\n",
    "#     with open(in_path, \"rb\") as f_in:\n",
    "#         data = pkl.load(f_in)\n",
    "\n",
    "#     stack = [-1] \n",
    "\n",
    "#     for entry in data.record:\n",
    "#         print(\"ORIGINAL\", entry)\n",
    "#         if(entry['type'] == 'pop'):\n",
    "#             stack.pop()\n",
    "#         if(entry['type'] == 'merge'):\n",
    "#             stack.pop()\n",
    "#             stack.append(entry['from'])\n",
    "#         if(entry['type'] == 'subordinate'):\n",
    "#             stack.append(entry['from'])\n",
    "#         if(entry['type'] == 'discard'):\n",
    "#             entry['to'] = stack[-1]\n",
    "#         entry['to'] = \n",
    "\n",
    "#     with open(in_path, \"wb\") as f_out:\n",
    "#         pkl.dump(data, f_out)\n",
    "#         # above line is pretty dangerous, should wait until everything is ready\n",
    "\n",
    "\n",
    "# def lengthen_sentences(in_path, out_path):\n",
    "#     data = None\n",
    "#     print(in_path)\n",
    "#     with open(in_path, \"rb\") as f_in:\n",
    "#         data = pkl.load(f_in)\n",
    "\n",
    "#     stack = [-1] \n",
    "#     current_text = \"\"\n",
    "#     for entry in data.record:\n",
    "#         print(\"ORIGINAL\", entry)\n",
    "#         if(entry['type'] == 'pop'):\n",
    "#             stack.pop()\n",
    "#         if(entry['type'] == 'merge'):\n",
    "#             stack.pop()\n",
    "#             stack.append(entry['from'])\n",
    "#         if(entry['type'] == 'subordinate'):\n",
    "#             stack.append(entry['from'])\n",
    "#         if(entry['type'] == 'discard'):\n",
    "#             entry['to'] = stack[-1]\n",
    "#         entry['to'] = \n",
    "\n",
    "#     with open(out_path, \"wb\") as f_out:\n",
    "#         pkl.dump(data, f_out)\n",
    "#         # above line is pretty dangerous, should wait until everything is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_json(in_dir = \"data/pkl_fine\", out_dir = \"data/json/\", out_name = \"data_fine.json\"):\n",
    "    to_write = {\n",
    "        'buf_str': [],\n",
    "        'lbuf': [],\n",
    "        'rbuf': [],\n",
    "        'hbuf': [],\n",
    "        'boldbuf': [],\n",
    "        'italbuf': [],\n",
    "        'stk_str': [],\n",
    "        'lstk': [],\n",
    "        'rstk': [],\n",
    "        'hstk': [],\n",
    "        'boldstk': [],\n",
    "        'italstk': [],\n",
    "        'type': []\n",
    "    }\n",
    "\n",
    "    for file in os.listdir(in_dir):\n",
    "        full_path = f'{in_dir}/{file}'\n",
    "        print(full_path)\n",
    "        if(os.path.isfile(full_path)):\n",
    "            for entry in get_record(full_path):\n",
    "                for key, val in entry.items():\n",
    "                    to_write[key].append(val)\n",
    "                    # print(key, val)\n",
    "    # print(to_write)\n",
    "    print(f'Found {len(to_write[\"buf_str\"])} elements')\n",
    "    out_path = f'{out_dir}/{out_name}'  \n",
    "\n",
    "    with open(out_path, \"w\") as f_out:\n",
    "        json.dump(to_write, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pkl_fine/Yoon-current-cv-web.pkl\n",
      "Using laparams =  <LAParams: char_margin=2.0, line_margin=0.5, word_margin=0.1 all_texts=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:01, 23.30it/s]\n",
      "100%|██████████| 1996/1996 [00:00<00:00, 232009.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pkl_fine/cv (1).pkl\n",
      "Using laparams =  <LAParams: char_margin=2.0, line_margin=0.5, word_margin=0.1 all_texts=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:03,  5.20it/s]\n",
      "100%|██████████| 1163/1163 [00:00<00:00, 259135.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pkl_fine/BhattCV 221.pkl\n",
      "Using laparams =  <LAParams: char_margin=2.0, line_margin=0.5, word_margin=0.1 all_texts=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:01, 13.22it/s]\n",
      "100%|██████████| 1310/1310 [00:00<00:00, 239059.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pkl_fine/cv-amato.pkl\n",
      "Using laparams =  <LAParams: char_margin=2.0, line_margin=0.5, word_margin=0.1 all_texts=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:01, 24.60it/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 207659.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pkl_fine/cv-Qizheng2022.pkl\n",
      "Using laparams =  <LAParams: char_margin=2.0, line_margin=0.5, word_margin=0.1 all_texts=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  6.45it/s]\n",
      "100%|██████████| 206/206 [00:00<00:00, 206457.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pkl_fine/vita_web.pkl\n",
      "Using laparams =  <LAParams: char_margin=2.0, line_margin=0.5, word_margin=0.1 all_texts=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 24.34it/s]\n",
      "100%|██████████| 753/753 [00:00<00:00, 245534.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pkl_fine/cv.pkl\n",
      "Using laparams =  <LAParams: char_margin=2.0, line_margin=0.5, word_margin=0.1 all_texts=False>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:04,  4.91it/s]\n",
      "100%|██████████| 1607/1607 [00:00<00:00, 258564.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9535 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pkl_to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff3b2c9d663614f3977b65735ceb4b5f1b8eea6ba2762bc66636352610c8026c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
